{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Listwise Learning to Rank Approach using BoltzRank.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbokUxrYW6JU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995e309b-aca6-4890-8b3d-0a2f56045739"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext cython\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import lightgbm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_svmlight_file \n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "The cython extension is already loaded. To reload it, use:\n",
            "  %reload_ext cython\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyRYejGKXKU6"
      },
      "source": [
        "## **LOAD DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdpjLQR_W86u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea3cb3d-fd01-43fd-e82e-582381156821"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIzPOzkvXFhw"
      },
      "source": [
        "DATASET_FOLDER = (\"/content/drive/MyDrive/PROYEK STBI/MQ2008/Fold1\")\n",
        "PERM_FOLDER = DATASET_FOLDER + \"perms/\"\n",
        "METRIC_NAME = 'ndcg@10'"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8I_CzcVZ7Eg"
      },
      "source": [
        "def ensureFile(path):\n",
        "    if not os.path.exists(path) or not os.path.isfile(path):\n",
        "        raise FileNotFoundError(\"'\" + path + \"': no such file\")        \n",
        "    return path\n",
        "\n",
        "def retrieveFileNames():\n",
        "    folder = DATASET_FOLDER + '/' if DATASET_FOLDER[-1:] != '/' else DATASET_FOLDER\n",
        "    train_file = ensureFile(folder + \"train.txt\")\n",
        "    valid_file = ensureFile(folder + \"vali.txt\")\n",
        "    test_file = ensureFile(folder + \"test.txt\")\n",
        "    return train_file, valid_file, test_file\n",
        "\n",
        "def loadDataset(path):\n",
        "    return load_svmlight_file(path, query_id=True)\n",
        "\n",
        "def loadLightGBM(svmlight_dataset):\n",
        "    query_lens = [sum(1 for _ in group) for key, group in itertools.groupby(svmlight_dataset[2])]\n",
        "    return lightgbm.Dataset(data=svmlight_dataset[0], label=svmlight_dataset[1], group=query_lens)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpIO4mekaCdC"
      },
      "source": [
        "class Query:\n",
        "    def __init__(self, qid):\n",
        "        self.qid = qid\n",
        "        self.labels_to_docs = {}\n",
        "    def addlabel(self, label):\n",
        "        if not label in self.labels_to_docs:\n",
        "            self.labels_to_docs[label] = list()\n",
        "    def adddoc(self, label, doc):\n",
        "        self.labels_to_docs[label].append(doc)\n",
        "    def finalize(self, alllabels):\n",
        "        self.labels = np.zeros(len(self.labels_to_docs.keys()), dtype=int)\n",
        "        self.docs = np.empty(len(self.labels_to_docs.keys()), dtype=object)\n",
        "        i = 0\n",
        "        totaldocs = 0\n",
        "        sorteddict = sorted(self.labels_to_docs.items(), reverse = True)\n",
        "        for label, docs in sorteddict:\n",
        "            self.labels[i] = label\n",
        "            self.docs[i] = np.zeros(len(docs), dtype=int)\n",
        "            for j in range(len(docs)):\n",
        "                self.docs[i][j] = docs[j]\n",
        "            i += 1\n",
        "            totaldocs += len(docs)\n",
        "        self.alldocs = np.concatenate(self.docs)\n",
        "        self.flatlabels = np.zeros(totaldocs, dtype=np.double)\n",
        "        i = 0\n",
        "        for label, docs in sorteddict:\n",
        "            for j in range(len(docs)):\n",
        "                self.flatlabels[i] = label\n",
        "                i += 1       \n",
        "        k = min(10, len(self.alldocs))\n",
        "        self.idealdcg = dcg_k(self.alldocs, alllabels, k) \n",
        "        del self.labels_to_docs\n",
        "    def setperms(self, perms):\n",
        "        self.perms = perms\n",
        "    def setndcgs(self, ndcgs):\n",
        "        self.ndcgs = ndcgs\n",
        "    def __repr__(self):  \n",
        "        return str(self)\n",
        "    def __str__(self):\n",
        "        res = \"Query \" + str(self.qid) + \"[\"\n",
        "        res += \"\\nideal dcg: \" + str(self.idealdcg)\n",
        "        for i in range(len(self.labels)):\n",
        "            res += \"\\n\" + str(self.labels[i]) + \" -> \" + str(self.docs[i])\n",
        "        res += \"]\"\n",
        "        if hasattr(self, 'perms'):\n",
        "            for i in range(len(self.perms)):\n",
        "                res += \"\\n[\" + str(self.perms[i]) + \"] -> dcg: \" + str(self.ndcgs[i])\n",
        "        else:\n",
        "            res += \"\\nNo permutations computed yet\"\n",
        "        return res"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40T4A25FaFdk"
      },
      "source": [
        "def mapQueryToDocuments(dataset):\n",
        "    queries = {}\n",
        "    alllabels = np.negative(np.ones(len(dataset[2]), dtype=np.double))\n",
        "    for i in range(0, len(dataset[2])):\n",
        "        if not dataset[2][i] in queries:\n",
        "            queries[dataset[2][i]] = Query(dataset[2][i])\n",
        "        query = queries[dataset[2][i]]\n",
        "        query.addlabel(dataset[1][i])\n",
        "        query.adddoc(dataset[1][i], i)\n",
        "        alllabels[i] = dataset[1][i]\n",
        "        \n",
        "    for q in queries.values():\n",
        "        q.finalize(alllabels)\n",
        "    \n",
        "    return queries, alllabels"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkam9ltnVvRH"
      },
      "source": [
        "## **Rank sample set generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ7_QLcTVnU7",
        "outputId": "4ecc32b2-1946-4dfb-e043-fe59e360939f"
      },
      "source": [
        "%load_ext Cython"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Cython extension is already loaded. To reload it, use:\n",
            "  %reload_ext Cython\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RxjCc1PaI8A"
      },
      "source": [
        "\n",
        "%%cython \n",
        "from libc.math cimport exp\n",
        "from cython.parallel import prange\n",
        "from cython import boundscheck, wraparound, cdivision\n",
        "from libc.math cimport log2\n",
        "from math import factorial\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "from libc.stdio cimport printf\n",
        "from libc.stdlib cimport malloc\n",
        "from libc.stdlib cimport free\n",
        "\n",
        "@boundscheck(False)\n",
        "@wraparound(False)\n",
        "cdef double* E(int[:] R, double[:] S) nogil:\n",
        "    cdef int k, j, m = len(R)\n",
        "    cdef double* energy = <double *> malloc(m*sizeof(double))\n",
        "    cdef double res_w_S, factor \n",
        "    if m == 1 or m == 0:\n",
        "        for j in prange(len(R), schedule='static', num_threads=8):\n",
        "            energy[j] = 0\n",
        "    else:\n",
        "        factor = 4.0 / (m * ((m - 1)**2))\n",
        "        for j in prange(len(R), schedule='static', num_threads=8):\n",
        "            res_w_S = 0.0\n",
        "            for k in range(len(R)):\n",
        "                if k < j: \n",
        "                    res_w_S = res_w_S + (j - k) * (S[R[j]] - S[R[k]])\n",
        "                elif k > j: \n",
        "                    res_w_S = res_w_S + (k - j) * (S[R[k]] - S[R[j]])\n",
        "            energy[j] = factor * res_w_S\n",
        "    return energy\n",
        "\n",
        "@boundscheck(False)\n",
        "@wraparound(False)\n",
        "cdef void P(int[:,:] Rq, double[:] S, double[:,:] probs, double[:] accumulator) nogil:\n",
        "    cdef int rankid, pos, doc\n",
        "    cdef double* en\n",
        "    for rankid in prange(len(Rq), schedule='static', num_threads=8):\n",
        "        en = E(Rq[rankid], S)\n",
        "        for pos in range(len(Rq[rankid])):\n",
        "            doc = Rq[rankid][pos]\n",
        "            probs[doc][rankid] = exp(-en[pos]) # e^{-E}\n",
        "            accumulator[doc] = accumulator[doc] + probs[doc][rankid] # sum(e^{-E})\n",
        "        free(en)\n",
        "    for pos in prange(len(Rq[0]), schedule='static', num_threads=8):\n",
        "        doc = Rq[0][pos]\n",
        "        # e^{-E} / sum(e^{-E})\n",
        "        probs[doc][rankid] = probs[doc][rankid] / accumulator[doc]\n",
        "        \n",
        "\n",
        "#NDCG EVALUATION\n",
        "\n",
        "@boundscheck(False)\n",
        "@wraparound(False)\n",
        "cpdef double dcg_k(int[:] rank, double[:] scores, int k) nogil:\n",
        "    cdef double result = 0\n",
        "    cdef int i\n",
        "    for i in prange(k, schedule='static', num_threads=8):\n",
        "        result += (2**scores[rank[i]] - 1) / (log2(i + 2)) # should be i+1, but with numbering starting from 1 instead of 0\n",
        "    return result\n",
        "\n",
        "\n",
        "@boundscheck(False)\n",
        "@wraparound(False)\n",
        "cdef double ndcg_k(int[:] rank, double[:] scores, int k, double ideal) nogil:\n",
        "    if ideal == 0:\n",
        "        return 1.0\n",
        "    return dcg_k(rank, scores, k) / ideal\n",
        "\n",
        "\n",
        "#PERMUTATIONS GENERATION\n",
        "\n",
        "RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS = 100\n",
        "RANK_SAMPLE_SET_DISTRIBUTIONS = [\n",
        "                                int(.30 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->0\n",
        "                                int(.22 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->1\n",
        "                                int(.18 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->2\n",
        "                                int(.12 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->3\n",
        "                                int(.10 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->0\n",
        "                                int(.06 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->1\n",
        "                                int(.02 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->2\n",
        "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 2->0\n",
        "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 2->1\n",
        "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS)  # 1->0\n",
        "                                ]\n",
        "\n",
        "@boundscheck(False)\n",
        "@wraparound(False)\n",
        "cdef bint contained(int[:,:] container, int[:] array) nogil:\n",
        "    cdef bint match\n",
        "    cdef int i\n",
        "    cdef int j\n",
        "    for i in prange(len(container), schedule='static', num_threads=8):\n",
        "        if container[i][0] == -1 or len(container[i]) != len(array):\n",
        "            continue\n",
        "        else:\n",
        "            match = True\n",
        "            for j in range(len(container[i])):\n",
        "                if container[i][j] != array[j]:\n",
        "                    match = False\n",
        "                    break\n",
        "            if match:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "@boundscheck(False)\n",
        "@wraparound(False)\n",
        "cdef void setrow(int[:,:] container, int pos, int[:] array) nogil:\n",
        "    cdef int i\n",
        "    for i in prange(len(container[pos]), schedule='static', num_threads=8):\n",
        "        container[pos][i] = array[i]\n",
        "\n",
        "@boundscheck(False)\n",
        "@wraparound(False)\n",
        "cdef int[:,:] allPerms(int[:] source, long long fact):\n",
        "    cdef int i = 0\n",
        "    cdef int k\n",
        "    perm = itertools.permutations(source)\n",
        "    cdef int[:,:] result = np.zeros((fact, len(source)), dtype=int)\n",
        "    for p in perm:\n",
        "        for k in range(len(p)):\n",
        "            result[i][k] = p[k]\n",
        "        i += 1\n",
        "    return result\n",
        "\n",
        "#source: label -> docid*, i: int, j: int, count: int, perms_with_prob: tuple<int> -> float\n",
        "#return: number of not computed permutations\n",
        "@boundscheck(False)\n",
        "@wraparound(False)\n",
        "def perform_permutation(query, int i, int j, int count, int[:,:] perms, int start):\n",
        "    if not i in query.labels or not j in query.labels:\n",
        "        # no swapping possible\n",
        "        return count, start\n",
        "    # find the indexes of the desired labels\n",
        "    i = [k for k in range(len(query.labels)) if query.labels[k] == i][0]\n",
        "    j = [k for k in range(len(query.labels)) if query.labels[k] == j][0]\n",
        "    cdef int c = 0\n",
        "    cdef int _min = min(len(query.docs[i]), len(query.docs[j]))\n",
        "    cdef int amount = max(1, int(_min * .5))\n",
        "    limit = factorial(_min) / (factorial(amount) * factorial(_min - amount))\n",
        "    cdef int k\n",
        "    cdef int d\n",
        "    for k in range(count):\n",
        "        perm = query.docs.copy()\n",
        "        first = random.sample(range(len(query.docs[i])), k=amount)\n",
        "        second = random.sample(range(len(query.docs[j])), k=amount)\n",
        "        for d in range(len(first)):\n",
        "            perm[i][first[d]], perm[j][second[d]] = query.docs[j][second[d]], query.docs[i][first[d]]\n",
        "        p = np.concatenate(perm)\n",
        "        if not contained(perms, p):\n",
        "            setrow(perms, start + c, p)\n",
        "            c += 1\n",
        "            if c == limit:\n",
        "                return count - c, start + c\n",
        "        else:\n",
        "            k -= 1\n",
        "    return 0, start + c\n",
        "\n",
        "@boundscheck(False)\n",
        "@wraparound(False)\n",
        "def process_query(query, alllabels, probs, accumulator):\n",
        "    cdef int carry = 0\n",
        "    fact = factorial(len(query.alldocs))\n",
        "    cdef perms\n",
        "    cdef int last = 0\n",
        "    if fact <= RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS:\n",
        "        # evaluate all possible permutations, each one representing a different ranking\n",
        "        perms = allPerms(query.alldocs, fact)\n",
        "    else:\n",
        "        perms = np.negative(np.ones((RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS, len(query.alldocs)), dtype=int))\n",
        "        # switch the labels of the documents, then sort the documents by label to obtain a ranking\n",
        "        carry, last = perform_permutation(query, 4, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[0], perms, last)\n",
        "        carry, last = perform_permutation(query, 4, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[1] + carry, perms, last)\n",
        "        carry, last = perform_permutation(query, 4, 2, RANK_SAMPLE_SET_DISTRIBUTIONS[2] + carry, perms, last)\n",
        "        carry, last = perform_permutation(query, 4, 3, RANK_SAMPLE_SET_DISTRIBUTIONS[3] + carry, perms, last)\n",
        "        carry, last = perform_permutation(query, 3, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[4] + carry, perms, last)\n",
        "        carry, last = perform_permutation(query, 3, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[5] + carry, perms, last)\n",
        "        carry, last = perform_permutation(query, 3, 2, RANK_SAMPLE_SET_DISTRIBUTIONS[6] + carry, perms, last)\n",
        "        carry, last = perform_permutation(query, 2, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[7] + carry, perms, last)\n",
        "        carry, last = perform_permutation(query, 2, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[8] + carry, perms, last)\n",
        "        carry, last = perform_permutation(query, 1, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[9] + carry, perms, last)\n",
        "        if carry != 0:\n",
        "            if not query.alldocs in perms:\n",
        "                perms[last] = query.alldocs\n",
        "        perms = perms[perms.max(axis=1)>=0]\n",
        "    query.setperms(perms)  \n",
        "    P(perms, alllabels, probs, accumulator)\n",
        "    cdef double[:] ndcgs = np.zeros(len(perms))\n",
        "    cdef int k = min(10, len(perms[0]))\n",
        "    for i in range(len(perms)):\n",
        "        ndcgs[i] = ndcg_k(perms[i], alllabels, k, query.idealdcg)\n",
        "    query.setndcgs(ndcgs)\n",
        "    return query"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9PMJUQRU2oH",
        "outputId": "37ff3e7a-994c-4891-a623-132e98a6dfcb"
      },
      "source": [
        "train_file, valid_file, test_file = retrieveFileNames()\n",
        "\n",
        "print(\"training file: \" + train_file)\n",
        "print(\"validation file: \" + valid_file)\n",
        "print(\"test file: \" + test_file)\n",
        "    \n",
        "print(\"loading datasets... \")\n",
        "import time\n",
        "start = time.process_time()\n",
        "train_dataset = loadDataset(train_file)\n",
        "print(\"train dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
        "start = time.process_time()\n",
        "valid_dataset = loadDataset(valid_file)\n",
        "print(\"validation dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
        "start = time.process_time()\n",
        "test_dataset = loadDataset(test_file)\n",
        "print(\"test dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
        "\n",
        "import itertools\n",
        "print(\"converting datasets to LightGBM format... \")\n",
        "train_lgb = loadLightGBM(train_dataset)\n",
        "valid_lgb = loadLightGBM(valid_dataset)\n",
        "test_lgb = loadLightGBM(test_dataset)\n",
        "\n",
        "print(\"done\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training file: /content/drive/MyDrive/PROYEK STBI/MQ2008/Fold1/train.txt\n",
            "validation file: /content/drive/MyDrive/PROYEK STBI/MQ2008/Fold1/vali.txt\n",
            "test file: /content/drive/MyDrive/PROYEK STBI/MQ2008/Fold1/test.txt\n",
            "loading datasets... \n",
            "train dataset loading took 0.30285060999999835 s\n",
            "validation dataset loading took 0.09325055100000057 s\n",
            "test dataset loading took 0.08717540700000015 s\n",
            "converting datasets to LightGBM format... \n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciQm5bE9V5Oo",
        "outputId": "1f2cfb88-b363-4100-c3e3-20e73ba31ee3"
      },
      "source": [
        "\n",
        "def file_len(fname):\n",
        "    with open(fname) as f:\n",
        "        for i, l in enumerate(f):\n",
        "            pass\n",
        "    return i + 1\n",
        "\n",
        "print(\"creating query-documents mappings...\")\n",
        "train_id = file_len(train_file)\n",
        "vali_id = file_len(valid_file)\n",
        "test_id = file_len(test_file)\n",
        "ds_to_queries = {}\n",
        "ds_to_queries[train_id] = mapQueryToDocuments(train_dataset)\n",
        "ds_to_queries[vali_id] = mapQueryToDocuments(valid_dataset)\n",
        "ds_to_queries[test_id] = mapQueryToDocuments(test_dataset)\n",
        "print(\"done\")\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating query-documents mappings...\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlUsLXmocies",
        "outputId": "d74b3644-9e1c-4550-8c1a-1990a807c3bb"
      },
      "source": [
        "\n",
        "print(\"creating sample sets...\")\n",
        "start = time.process_time()\n",
        "\n",
        "probs_with_labels = {}\n",
        "RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS = 100\n",
        "for ds_id, queries in ds_to_queries.items():\n",
        "    probs_with_labels[ds_id] = np.zeros((len(queries[1]), RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS))\n",
        "    accumulator = np.zeros(len(queries[1]))\n",
        "    for q in queries[0].values():\n",
        "        process_query(q, queries[1], probs_with_labels[ds_id], accumulator)    \n",
        "    del accumulator\n",
        "    \n",
        "print(\"sample set creation took \" + str(time.process_time() - start) + \" s\")\n",
        "print(\"done\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating sample sets...\n",
            "sample set creation took 0.0001798969999988742 s\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ldUGKWsdiPW"
      },
      "source": [
        "## **BoltzRank logic**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgVtuAKude2q"
      },
      "source": [
        ""
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo6h0us8dmy1"
      },
      "source": [
        ""
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d39eG3QcdrX5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}